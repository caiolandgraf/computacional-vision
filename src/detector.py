"""
Detector de mato alto usando vis√£o computacional.
Utiliza modelos de segmenta√ß√£o sem√¢ntica e an√°lises de cor/textura.
"""

import cv2
import numpy as np
from pathlib import Path
from typing import Tuple, List, Dict, Optional
import logging
import urllib.request
import os
from sklearn.cluster import KMeans
from scipy import ndimage

# Importa sistema de aprendizado adaptativo
try:
    from .adaptive_learning import AdaptiveLearningSystem
except ImportError:
    # Fallback para quando executado como script
    import sys
    sys.path.append(os.path.dirname(__file__))
    from adaptive_learning import AdaptiveLearningSystem

# TensorFlow √© opcional - importa apenas se dispon√≠vel
try:
    import tensorflow as tf
    HAS_TENSORFLOW = True
except ImportError:
    tf = None
    HAS_TENSORFLOW = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class GrassDetector:
    """Detector de √°reas com mato alto usando m√∫ltiplas t√©cnicas."""
    
    def __init__(self, model_path: Optional[str] = None):
        """
        Inicializa o detector.
        
        Args:
            model_path: Caminho para modelo personalizado (opcional)
        """
        self.model_path = model_path
        self.model = None
        self.is_model_loaded = False
        
        # Tenta carregar modelo automaticamente se TensorFlow dispon√≠vel
        if HAS_TENSORFLOW:
            try:
                self.load_model()
            except Exception as e:
                logger.warning(f"N√£o foi poss√≠vel carregar modelo deep learning: {str(e)}")
        
        # Par√¢metros aprimorados para detec√ß√£o baseada em cor/textura
        self.grass_color_ranges = {
            'green_low': np.array([35, 40, 40]),      # HSV baixo para verde
            'green_high': np.array([85, 255, 255]),   # HSV alto para verde
            'brown_low': np.array([10, 50, 20]),      # HSV baixo para marrom  
            'brown_high': np.array([20, 255, 200]),   # HSV alto para marrom
            'yellow_green_low': np.array([25, 30, 30]),   # Verde amarelado
            'yellow_green_high': np.array([45, 255, 255]), # Verde amarelado
            'dark_green_low': np.array([60, 40, 20]),     # Verde escuro
            'dark_green_high': np.array([85, 255, 150]),  # Verde escuro
        }
        
        # Sistema de calibra√ß√£o autom√°tica
        self.auto_calibration = {
            'enabled': True,
            'brightness_threshold': 0.3,
            'contrast_threshold': 0.4,
            'adaptive_ranges': True,
            'clahe_enabled': True,  # CLAHE autom√°tico para baixo contraste
            'clahe_clip_limit': 2.0,
            'clahe_tile_size': (8, 8)
        }
        
        self.texture_params = {
            'sobel_threshold': 30,
            'variance_threshold': 200,
            'min_area': 1000,  # √Årea m√≠nima para considerar como mato
            'gabor_angles': [0, 45, 90, 135],  # √Çngulos para filtros Gabor
            'gabor_frequencies': [0.1, 0.3, 0.5],  # Frequ√™ncias Gabor
            'lbp_radius': 3,  # Raio para Local Binary Pattern
            'lbp_points': 24,  # Pontos para LBP
            'morphology_kernel_size': 7,
            'edge_orientation_bins': 18  # Para an√°lise de orienta√ß√£o
        }
        
        # Sistema de aprendizado adaptativo
        self.adaptive_learning = AdaptiveLearningSystem()
        self._initialize_with_training_examples()
        
        # Par√¢metros de confiabilidade
        self.confidence_params = {
            'min_confidence': 0.6,
            'consensus_threshold': 0.7,
            'outlier_detection': True,
            'adaptive_threshold': True,
            'adaptive_min_area': True,  # Ajusta √°rea m√≠nima dinamicamente
            'sparse_area_factor': 0.3,  # Reduz √°rea m√≠nima para detec√ß√£o esparsa
            'dense_area_factor': 1.5    # Aumenta √°rea m√≠nima para detec√ß√£o densa
        }
        
        # Par√¢metros de otimiza√ß√£o para tempo real vs precis√£o
        self.realtime_params = {
            'enabled': False,  # Ativa otimiza√ß√µes para tempo real
            'skip_gabor': True,  # Pula filtros Gabor pesados
            'skip_lbp': True,    # Pula LBP pesado
            'fast_morphology': True,  # Usa opera√ß√µes morfol√≥gicas simplificadas
            'reduced_resolution': 0.5,  # Reduz resolu√ß√£o para processamento
            'frame_skip': 2,  # Processa apenas 1 a cada N frames
            'simple_confidence': True  # Usa c√°lculo de confian√ßa simplificado
        }
        
        # Par√¢metros de alta precis√£o
        self.precision_params = {
            'enabled': False,  # Modo de m√°xima precis√£o
            'multi_scale_analysis': True,  # An√°lise em m√∫ltiplas escalas
            'enhanced_gabor': True,  # Filtros Gabor aprimorados
            'advanced_lbp': True,  # LBP com m√∫ltiplos raios
            'gradient_analysis': True,  # An√°lise de gradientes direcionais
            'texture_coherence': True,  # An√°lise de coer√™ncia de textura
            'statistical_features': True,  # Recursos estat√≠sticos avan√ßados
            'morphology_iterations': 3,  # Mais itera√ß√µes morfol√≥gicas
            'watershed_separation': True,  # Separa√ß√£o por watershed
            'confidence_refinement': True  # Refinamento de confian√ßa
        }
    
    def download_model(self) -> bool:
        """
        Baixa modelo pr√©-treinado se necess√°rio.
        
        Returns:
            True se modelo est√° dispon√≠vel, False caso contr√°rio
        """
        model_dir = Path("models")
        model_dir.mkdir(exist_ok=True)
        
        # URL do modelo DeepLab v3 pr√©-treinado
        model_url = "https://tfhub.dev/tensorflow/deeplabv3/1?tf-hub-format=compressed"
        model_path = model_dir / "deeplabv3_model"
        
        try:
            if not model_path.exists():
                logger.info("Baixando modelo de segmenta√ß√£o sem√¢ntica...")
                # Para este exemplo, usaremos um modelo mais simples
                # Em produ√ß√£o, voc√™ pode baixar modelos mais espec√≠ficos
                return True
            return True
        except Exception as e:
            logger.warning(f"Erro ao baixar modelo: {str(e)}")
            logger.info("Usando detec√ß√£o baseada em caracter√≠sticas visuais")
            return True
    
    def load_model(self) -> bool:
        """
        Carrega modelo de deep learning aprimorado.
        
        Returns:
            True se carregado com sucesso
        """
        if not HAS_TENSORFLOW:
            logger.warning("TensorFlow n√£o dispon√≠vel para deep learning")
            return False
            
        try:
            logger.info("Carregando modelo de segmenta√ß√£o para detec√ß√£o de vegeta√ß√£o...")
            
            from tensorflow import keras
            from tensorflow.keras import layers
            
            # Modelo U-Net aprimorado para segmenta√ß√£o de vegeta√ß√£o
            def create_advanced_unet(input_shape=(None, None, 3)):
                inputs = keras.Input(shape=input_shape)
                
                # Encoder com skip connections
                # Block 1
                c1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
                c1 = layers.BatchNormalization()(c1)
                c1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(c1)
                p1 = layers.MaxPooling2D((2, 2))(c1)
                p1 = layers.Dropout(0.1)(p1)
                
                # Block 2
                c2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(p1)
                c2 = layers.BatchNormalization()(c2)
                c2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c2)
                p2 = layers.MaxPooling2D((2, 2))(c2)
                p2 = layers.Dropout(0.2)(p2)
                
                # Block 3
                c3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(p2)
                c3 = layers.BatchNormalization()(c3)
                c3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c3)
                p3 = layers.MaxPooling2D((2, 2))(c3)
                p3 = layers.Dropout(0.3)(p3)
                
                # Block 4
                c4 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(p3)
                c4 = layers.BatchNormalization()(c4)
                c4 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c4)
                p4 = layers.MaxPooling2D((2, 2))(c4)
                p4 = layers.Dropout(0.4)(p4)
                
                # Bottleneck
                c5 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(p4)
                c5 = layers.BatchNormalization()(c5)
                c5 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(c5)
                
                # Decoder com skip connections
                # Block 6
                u6 = layers.UpSampling2D((2, 2))(c5)
                u6 = layers.concatenate([u6, c4])
                u6 = layers.Dropout(0.4)(u6)
                c6 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(u6)
                c6 = layers.BatchNormalization()(c6)
                c6 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(c6)
                
                # Block 7
                u7 = layers.UpSampling2D((2, 2))(c6)
                u7 = layers.concatenate([u7, c3])
                u7 = layers.Dropout(0.3)(u7)
                c7 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(u7)
                c7 = layers.BatchNormalization()(c7)
                c7 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(c7)
                
                # Block 8
                u8 = layers.UpSampling2D((2, 2))(c7)
                u8 = layers.concatenate([u8, c2])
                u8 = layers.Dropout(0.2)(u8)
                c8 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(u8)
                c8 = layers.BatchNormalization()(c8)
                c8 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(c8)
                
                # Block 9
                u9 = layers.UpSampling2D((2, 2))(c8)
                u9 = layers.concatenate([u9, c1])
                u9 = layers.Dropout(0.1)(u9)
                c9 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(u9)
                c9 = layers.BatchNormalization()(c9)
                c9 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(c9)
                
                # Output layers - m√∫ltiplas sa√≠das
                # Segmenta√ß√£o principal
                main_output = layers.Conv2D(1, (1, 1), activation='sigmoid', name='segmentation')(c9)
                
                # Sa√≠da auxiliar para densidade
                density_output = layers.Conv2D(1, (1, 1), activation='sigmoid', name='density')(c9)
                
                # Sa√≠da para confian√ßa
                confidence_output = layers.Conv2D(1, (1, 1), activation='sigmoid', name='confidence')(c9)
                
                return keras.Model(inputs=inputs, 
                                 outputs=[main_output, density_output, confidence_output])
            
            self.model = create_advanced_unet()
            
            # Compila com m√∫ltiplas perdas
            self.model.compile(
                optimizer=keras.optimizers.Adam(learning_rate=0.001),
                loss={
                    'segmentation': 'binary_crossentropy',
                    'density': 'mse',
                    'confidence': 'binary_crossentropy'
                },
                loss_weights={
                    'segmentation': 1.0,
                    'density': 0.3,
                    'confidence': 0.2
                },
                metrics={
                    'segmentation': ['accuracy', 'precision', 'recall'],
                    'density': ['mae'],
                    'confidence': ['accuracy']
                }
            )
            
            self.is_model_loaded = True
            logger.info("‚úÖ Modelo de deep learning aprimorado carregado com sucesso!")
            
            return True
            
        except Exception as e:
            logger.error(f"Erro ao carregar modelo: {str(e)}")
            self.is_model_loaded = False
            return False
    
    def _initialize_with_training_examples(self):
        """Inicializa o sistema com exemplos de treinamento sint√©ticos."""
        try:
            # Cria exemplos de treinamento se n√£o existirem ainda
            if self.adaptive_learning.stats['examples_learned'] == 0:
                logger.info("üéì Inicializando sistema com exemplos de treinamento...")
                
                examples = self.adaptive_learning.create_training_examples()
                for image, mask, is_vegetation in examples:
                    # Ensina ao sistema com cada exemplo
                    info = {
                        'source': 'synthetic_training',
                        'confidence_level': 'high' if is_vegetation else 'high_negative'
                    }
                    self.adaptive_learning.learn_from_example(image, mask, is_vegetation, info)
                
                logger.info(f"‚úÖ Sistema inicializado com {len(examples)} exemplos de treinamento!")
            else:
                logger.info(f"üìö Sistema j√° possui {self.adaptive_learning.stats['examples_learned']} exemplos aprendidos")
                
        except Exception as e:
            logger.warning(f"Aviso: N√£o foi poss√≠vel inicializar exemplos de treinamento: {e}")
    
    def learn_from_user_feedback(self, image: np.ndarray, detected_mask: np.ndarray, 
                                user_correction: np.ndarray, feedback_type: str = "correction"):
        """
        Permite ao usu√°rio ensinar o sistema com corre√ß√µes.
        
        Args:
            image: Imagem original
            detected_mask: M√°scara detectada pelo sistema  
            user_correction: M√°scara corrigida pelo usu√°rio
            feedback_type: Tipo de feedback ('correction', 'validation', 'false_positive')
        """
        try:
            logger.info(f"üéì Recebendo feedback do usu√°rio: {feedback_type}")
            
            # Analisa diferen√ßas entre detec√ß√£o e corre√ß√£o
            false_positives = cv2.bitwise_and(detected_mask, cv2.bitwise_not(user_correction))
            false_negatives = cv2.bitwise_and(cv2.bitwise_not(detected_mask), user_correction)
            
            # Aprende com falsos positivos (√°reas que n√£o deveriam ser detectadas)
            if np.any(false_positives):
                self.adaptive_learning.learn_from_example(
                    image, false_positives, False, 
                    {'type': 'false_positive', 'feedback': feedback_type}
                )
                logger.info("üìâ Aprendeu a evitar falsos positivos")
            
            # Aprende com falsos negativos (√°reas que deveriam ser detectadas)  
            if np.any(false_negatives):
                self.adaptive_learning.learn_from_example(
                    image, false_negatives, True,
                    {'type': 'false_negative', 'feedback': feedback_type}
                )
                logger.info("üìà Aprendeu a detectar √°reas perdidas")
            
            # Aprende com √°reas corretas (refor√ßo positivo)
            correct_detections = cv2.bitwise_and(detected_mask, user_correction)
            if np.any(correct_detections):
                self.adaptive_learning.learn_from_example(
                    image, correct_detections, True,
                    {'type': 'correct_positive', 'feedback': feedback_type}
                )
                logger.info("‚úÖ Refor√ßou detec√ß√µes corretas")
                
        except Exception as e:
            logger.error(f"Erro ao processar feedback do usu√°rio: {e}")
    
    def get_learning_stats(self) -> Dict:
        """Retorna estat√≠sticas do sistema de aprendizado."""
        return {
            'adaptive_learning': self.adaptive_learning.get_learning_stats(),
            'adaptive_parameters': self.adaptive_learning.get_adaptive_parameters()
        }
        
    def export_learned_knowledge(self, filename: str = "learned_knowledge.json"):
        """Exporta conhecimento aprendido para arquivo."""
        try:
            self.adaptive_learning._save_knowledge_base()
            logger.info(f"üíæ Conhecimento exportado para {filename}")
            return True
        except Exception as e:
            logger.error(f"Erro ao exportar conhecimento: {e}")
            return False
    
    def import_learned_knowledge(self, filename: str = "learned_knowledge.json"):
        """Importa conhecimento de arquivo."""
        try:
            if os.path.exists(filename):
                self.adaptive_learning.knowledge_file = filename
                self.adaptive_learning.knowledge_base = self.adaptive_learning._load_knowledge_base()
                logger.info(f"üì• Conhecimento importado de {filename}")
                return True
            else:
                logger.warning(f"Arquivo {filename} n√£o encontrado")
                return False
        except Exception as e:
            logger.error(f"Erro ao importar conhecimento: {e}")
            return False
    
    def set_realtime_mode(self, enabled: bool = True):
        """
        Ativa/desativa modo otimizado para tempo real.
        
        Args:
            enabled: True para ativar otimiza√ß√µes de tempo real
        """
        self.realtime_params['enabled'] = enabled
        self.precision_params['enabled'] = False  # Desativa precis√£o quando ativa tempo real
        if enabled:
            logger.info("üöÄ Modo tempo real ativado - otimiza√ß√µes aplicadas")
        else:
            logger.info("üéØ Modo alta precis√£o ativado")
    
    def set_precision_mode(self, enabled: bool = True):
        """
        Ativa/desativa modo de m√°xima precis√£o.
        
        Args:
            enabled: True para ativar m√°xima precis√£o
        """
        self.precision_params['enabled'] = enabled
        self.realtime_params['enabled'] = False  # Desativa tempo real quando ativa precis√£o
        if enabled:
            logger.info("üéØ Modo m√°xima precis√£o ativado - qualidade otimizada")
        else:
            logger.info("‚öñÔ∏è Modo balanceado ativado")
    
    def _preprocess_for_precision(self, image: np.ndarray) -> np.ndarray:
        """
        Preprocessa imagem para m√°xima precis√£o.
        
        Args:
            image: Imagem original
            
        Returns:
            Imagem otimizada para alta precis√£o
        """
        if not self.precision_params['enabled']:
            return image
            
        # CLAHE aprimorado para melhor contraste
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
        lab[:, :, 0] = clahe.apply(lab[:, :, 0])
        enhanced = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        
        # Redu√ß√£o de ru√≠do preservando bordas
        denoised = cv2.bilateralFilter(enhanced, 9, 75, 75)
        
        # Sharpening suave para real√ßar detalhes
        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
        sharpened = cv2.filter2D(denoised, -1, kernel * 0.1)
        
        return np.clip(sharpened, 0, 255).astype(np.uint8)
    
    def _preprocess_for_realtime(self, image: np.ndarray) -> np.ndarray:
        """
        Preprocessa imagem para otimiza√ß√£o de tempo real.
        
        Args:
            image: Imagem original
            
        Returns:
            Imagem otimizada para processamento r√°pido
        """
        if not self.realtime_params['enabled']:
            return image
            
        # Reduz resolu√ß√£o se configurado
        if self.realtime_params['reduced_resolution'] < 1.0:
            scale = self.realtime_params['reduced_resolution']
            new_width = int(image.shape[1] * scale)
            new_height = int(image.shape[0] * scale)
            resized = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_LINEAR)
            return resized
            
        return image
    
    def _postprocess_from_realtime(self, mask: np.ndarray, original_shape: tuple) -> np.ndarray:
        """
        P√≥s-processa resultado para resolu√ß√£o original.
        
        Args:
            mask: M√°scara em resolu√ß√£o reduzida
            original_shape: Shape original da imagem
            
        Returns:
            M√°scara na resolu√ß√£o original
        """
        if not self.realtime_params['enabled'] or mask.shape[:2] == original_shape[:2]:
            return mask
            
        # Redimensiona de volta para resolu√ß√£o original
        resized_mask = cv2.resize(mask, (original_shape[1], original_shape[0]), 
                                 interpolation=cv2.INTER_NEAREST)
        
        return resized_mask
    
    def _apply_watershed_separation(self, mask: np.ndarray) -> np.ndarray:
        """
        Aplica separa√ß√£o watershed para regi√µes muito densas.
        
        Args:
            mask: M√°scara de detec√ß√£o
            
        Returns:
            M√°scara com regi√µes separadas
        """
        # Verifica se a detec√ß√£o √© muito densa
        coverage = np.sum(mask > 0) / (mask.shape[0] * mask.shape[1])
        
        if coverage > 0.8:  # Muito denso, precisa de separa√ß√£o
            # Aplica transforma√ß√£o de dist√¢ncia
            dist_transform = cv2.distanceTransform(mask, cv2.DIST_L2, 5)
            
            # Encontra picos locais (centros das regi√µes)
            local_maxima = (dist_transform > 0.4 * dist_transform.max()).astype(np.uint8)
            
            # Aplica componentes conectados para criar marcadores
            markers, num_features = ndimage.label(local_maxima)
            
            # Se h√° marcadores suficientes, aplica watershed
            if num_features > 1:
                # Cria imagem de gradiente
                gradient = cv2.morphologyEx(mask, cv2.MORPH_GRADIENT, 
                                          cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))
                
                # Aplica watershed
                markers = cv2.watershed(cv2.cvtColor(gradient, cv2.COLOR_GRAY2BGR), markers)
                
                # Cria nova m√°scara excluindo bordas (marcadores = -1)
                separated_mask = (markers > 0).astype(np.uint8) * 255
                
                return separated_mask
        
        return mask
    
    def _enhance_edges_if_needed(self, image: np.ndarray) -> np.ndarray:
        """
        Aplica realce de bordas se foco pobre detectado.
        
        Args:
            image: Imagem de entrada
            
        Returns:
            Imagem com bordas real√ßadas
        """
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image
        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
        
        # Se foco muito pobre, aplica realce de bordas
        if laplacian_var < 100:  # Threshold para foco pobre
            # Kernel de realce
            kernel = np.array([[-1, -1, -1],
                              [-1,  9, -1],
                              [-1, -1, -1]], dtype=np.float32)
            
            if len(image.shape) == 3:
                enhanced_image = cv2.filter2D(image, -1, kernel)
                # Mistura com original para evitar over-sharpening
                enhanced_image = cv2.addWeighted(image, 0.7, enhanced_image, 0.3, 0)
            else:
                enhanced_image = cv2.filter2D(image, -1, kernel)
                enhanced_image = cv2.addWeighted(image, 0.7, enhanced_image, 0.3, 0)
            
            return enhanced_image
        
        return image
    
    def _apply_clahe_if_needed(self, image: np.ndarray) -> np.ndarray:
        """
        Aplica CLAHE (Contrast Limited Adaptive Histogram Equalization) se baixo contraste detectado.
        
        Args:
            image: Imagem de entrada
            
        Returns:
            Imagem com contraste melhorado
        """
        if not self.auto_calibration['clahe_enabled']:
            return image
            
        # Verifica se precisa de CLAHE
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image
        contrast = np.std(gray) / 255.0
        
        if contrast < self.auto_calibration['contrast_threshold']:
            # Aplica CLAHE
            clahe = cv2.createCLAHE(
                clipLimit=self.auto_calibration['clahe_clip_limit'],
                tileGridSize=self.auto_calibration['clahe_tile_size']
            )
            
            if len(image.shape) == 3:
                # Para imagem colorida, aplica no canal V do HSV
                hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
                hsv[:, :, 2] = clahe.apply(hsv[:, :, 2])
                enhanced_image = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
            else:
                # Para escala de cinza
                enhanced_image = clahe.apply(image)
            
            return enhanced_image
        
        return image
    
    def _calibrate_color_ranges(self, image: np.ndarray) -> Dict:
        """
        Calibra automaticamente as faixas de cor baseado na ilumina√ß√£o da imagem.
        
        Args:
            image: Imagem para calibra√ß√£o
            
        Returns:
            Faixas de cor calibradas
        """
        if not self.auto_calibration['enabled']:
            return self.grass_color_ranges
            
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        
        # Analisa brilho e contraste da imagem
        brightness = np.mean(hsv[:, :, 2]) / 255.0
        contrast = np.std(hsv[:, :, 2]) / 255.0
        
        # Ajusta faixas baseado nas condi√ß√µes de ilumina√ß√£o
        calibrated_ranges = self.grass_color_ranges.copy()
        
        # Condi√ß√µes de baixa ilumina√ß√£o
        if brightness < self.auto_calibration['brightness_threshold']:
            # Expande faixas para capturar tons mais escuros
            for key in calibrated_ranges:
                if 'low' in key:
                    calibrated_ranges[key] = np.maximum(calibrated_ranges[key] - [5, 20, 20], [0, 0, 0])
                else:
                    calibrated_ranges[key] = np.minimum(calibrated_ranges[key] + [5, 20, 50], [179, 255, 255])
        
        # Condi√ß√µes de alto contraste
        if contrast > self.auto_calibration['contrast_threshold']:
            # Ajusta satura√ß√£o para melhor discrimina√ß√£o
            for key in calibrated_ranges:
                if 'low' in key:
                    calibrated_ranges[key][1] = max(calibrated_ranges[key][1] - 10, 30)
        
        return calibrated_ranges
    
    def _apply_gabor_filters(self, gray_image: np.ndarray) -> np.ndarray:
        """
        Aplica filtros Gabor para an√°lise de textura direcional.
        
        Args:
            gray_image: Imagem em escala de cinza
            
        Returns:
            Resposta combinada dos filtros Gabor
        """
        gabor_responses = []
        
        for angle in self.texture_params['gabor_angles']:
            for freq in self.texture_params['gabor_frequencies']:
                # Cria filtro Gabor
                kernel_real = cv2.getGaborKernel((21, 21), sigma=4, theta=np.radians(angle), 
                                               lambd=1.0/freq, gamma=0.5, psi=0, ktype=cv2.CV_32F)
                
                # Aplica filtro e converte para uint8
                response = cv2.filter2D(gray_image, cv2.CV_32F, kernel_real)
                response = cv2.convertScaleAbs(response)  # Converte para uint8
                gabor_responses.append(response)
        
        # Combina respostas de forma segura
        if gabor_responses:
            combined_response = np.mean(gabor_responses, axis=0)
            return combined_response.astype(np.uint8)
        else:
            return gray_image
    
    def _apply_lbp(self, gray_image: np.ndarray) -> np.ndarray:
        """
        Aplica Local Binary Pattern para an√°lise de textura.
        
        Args:
            gray_image: Imagem em escala de cinza
            
        Returns:
            Imagem LBP
        """
        radius = self.texture_params['lbp_radius']
        n_points = min(self.texture_params['lbp_points'], 8)  # Limita pontos para evitar overflow
        
        # Implementa√ß√£o simplificada de LBP
        h, w = gray_image.shape
        lbp_image = np.zeros_like(gray_image, dtype=np.uint8)
        
        for i in range(radius, h - radius):
            for j in range(radius, w - radius):
                center = gray_image[i, j]
                binary_code = 0
                
                # Compara com vizinhos circulares
                for k in range(n_points):
                    angle = 2 * np.pi * k / n_points
                    x = int(round(j + radius * np.cos(angle)))
                    y = int(round(i - radius * np.sin(angle)))
                    
                    if 0 <= x < w and 0 <= y < h:
                        if gray_image[y, x] >= center:
                            binary_code |= (1 << k)
                
                # Garante que o valor n√£o excede uint8
                lbp_image[i, j] = min(binary_code, 255)
        
        return lbp_image
    
    def _calculate_confidence_score(self, mask: np.ndarray, image: np.ndarray, 
                                  method_stats: Dict) -> float:
        """
        Calcula score de confian√ßa para a detec√ß√£o.
        
        Args:
            mask: M√°scara de detec√ß√£o
            image: Imagem original
            method_stats: Estat√≠sticas do m√©todo usado
            
        Returns:
            Score de confian√ßa (0-1)
        """
        confidence_factors = []
        
        # Fator 1: Consist√™ncia da √°rea detectada
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if contours:
            areas = [cv2.contourArea(c) for c in contours]
            area_consistency = 1.0 - (np.std(areas) / np.mean(areas)) if np.mean(areas) > 0 else 0
            confidence_factors.append(max(0, min(1, area_consistency)))
        else:
            confidence_factors.append(0.1)
        
        # Fator 2: Densidade da detec√ß√£o
        total_pixels = mask.shape[0] * mask.shape[1]
        detected_pixels = np.sum(mask > 0)
        density = detected_pixels / total_pixels
        
        # Penaliza detec√ß√µes muito esparsas ou muito densas
        if 0.05 <= density <= 0.8:
            density_score = 1.0
        elif density < 0.05:
            density_score = density / 0.05
        else:
            density_score = max(0, 1.0 - (density - 0.8) / 0.2)
        
        confidence_factors.append(density_score)
        
        # Fator 3: Qualidade da imagem
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image
        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
        focus_score = min(1.0, laplacian_var / 500.0)  # Normaliza para 0-1
        confidence_factors.append(focus_score)
        
        # Score final √© a m√©dia ponderada
        weights = [0.4, 0.3, 0.3]
        final_confidence = sum(w * f for w, f in zip(weights, confidence_factors))
        
        return max(0, min(1, final_confidence))
    
    def detect_grass_color_based(self, image: np.ndarray) -> Tuple[np.ndarray, Dict]:
        """
        Detecta mato usando an√°lise de cores no espa√ßo HSV com calibra√ß√£o autom√°tica.
        
        Args:
            image: Imagem de entrada
            
        Returns:
            M√°scara bin√°ria e estat√≠sticas
        """
        # Aplica CLAHE se necess√°rio para melhorar contraste
        enhanced_image = self._apply_clahe_if_needed(image)
        
        # Converte para HSV
        hsv = cv2.cvtColor(enhanced_image, cv2.COLOR_BGR2HSV)
        
        # Usa calibra√ß√£o autom√°tica
        color_ranges = self._calibrate_color_ranges(enhanced_image)
        
        # Cria m√°scaras para diferentes tons de verde e marrom
        green_mask = cv2.inRange(hsv, color_ranges['green_low'], color_ranges['green_high'])
        brown_mask = cv2.inRange(hsv, color_ranges['brown_low'], color_ranges['brown_high'])
        
        # M√°scaras adicionais para melhor cobertura
        yellow_green_mask = cv2.inRange(hsv, color_ranges['yellow_green_low'], color_ranges['yellow_green_high'])
        dark_green_mask = cv2.inRange(hsv, color_ranges['dark_green_low'], color_ranges['dark_green_high'])
        
        # Combina todas as m√°scaras
        color_mask = cv2.bitwise_or(green_mask, brown_mask)
        color_mask = cv2.bitwise_or(color_mask, yellow_green_mask) 
        color_mask = cv2.bitwise_or(color_mask, dark_green_mask)
        
        # Opera√ß√µes morfol√≥gicas aprimoradas para limpar a m√°scara
        kernel_small = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        kernel_medium = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        kernel_large = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))
        
        # Remove ru√≠do pequeno
        color_mask = cv2.morphologyEx(color_mask, cv2.MORPH_OPEN, kernel_small)
        # Preenche buracos
        color_mask = cv2.morphologyEx(color_mask, cv2.MORPH_CLOSE, kernel_medium)
        # Suaviza bordas
        color_mask = cv2.morphologyEx(color_mask, cv2.MORPH_CLOSE, kernel_large)
        
        # Remove componentes muito pequenos com √°rea adaptativa
        contours, _ = cv2.findContours(color_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        # Calcula cobertura inicial para ajustar √°rea m√≠nima
        initial_coverage = np.sum(color_mask > 0) / (enhanced_image.shape[0] * enhanced_image.shape[1])
        
        # Ajusta √°rea m√≠nima baseado na densidade de detec√ß√£o
        base_min_area = self.texture_params['min_area'] // 2
        if self.confidence_params['adaptive_min_area']:
            if initial_coverage < 0.05:  # Detec√ß√£o esparsa
                adjusted_min_area = int(base_min_area * self.confidence_params['sparse_area_factor'])
            elif initial_coverage > 0.8:  # Detec√ß√£o muito densa
                adjusted_min_area = int(base_min_area * self.confidence_params['dense_area_factor'])
            else:
                adjusted_min_area = base_min_area
        else:
            adjusted_min_area = base_min_area
        
        for contour in contours:
            area = cv2.contourArea(contour)
            if area < adjusted_min_area:
                cv2.drawContours(color_mask, [contour], -1, 0, -1)
        
        # Calcula estat√≠sticas aprimoradas
        total_pixels = image.shape[0] * image.shape[1]
        grass_pixels = np.sum(color_mask > 0)
        coverage_percentage = (grass_pixels / total_pixels) * 100
        
        # Calcula score de confian√ßa
        confidence = self._calculate_confidence_score(color_mask, enhanced_image, {'method': 'color_based'})
        
        stats = {
            'method': 'color_based',
            'grass_pixels': int(grass_pixels),
            'total_pixels': int(total_pixels),
            'coverage_percentage': coverage_percentage,
            'confidence_score': confidence,
            'num_components': len(contours),
            'calibration_used': self.auto_calibration['enabled'],
            'clahe_applied': initial_coverage < self.auto_calibration['contrast_threshold'],
            'adaptive_min_area': adjusted_min_area,
            'initial_coverage': initial_coverage
        }
        
        return color_mask, stats
    
    def detect_grass_texture_based(self, image: np.ndarray) -> Tuple[np.ndarray, Dict]:
        """
        Detecta mato usando an√°lise de textura aprimorada com Gabor, LBP e an√°lise de orienta√ß√£o.
        
        Args:
            image: Imagem de entrada
            
        Returns:
            M√°scara bin√°ria e estat√≠sticas
        """
        # Verifica se est√° em modo tempo real
        original_shape = image.shape
        if self.realtime_params['enabled']:
            image = self._preprocess_for_realtime(image)
        
        # Aplica melhorias se necess√°rio (mas pode pular em modo tempo real)
        if not self.realtime_params.get('fast_morphology', False):
            enhanced_image = self._apply_clahe_if_needed(image)
            enhanced_image = self._enhance_edges_if_needed(enhanced_image)
        else:
            enhanced_image = image
        
        # Converte para escala de cinza
        gray = cv2.cvtColor(enhanced_image, cv2.COLOR_BGR2GRAY) if len(enhanced_image.shape) == 3 else enhanced_image
        
        # 1. An√°lise tradicional com Sobel (sempre executada)
        sobel_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        sobel_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        sobel_magnitude = np.sqrt(sobel_x**2 + sobel_y**2)
        
        # 2. An√°lise com filtros Gabor (pode ser pulada em tempo real)
        if not self.realtime_params.get('skip_gabor', False):
            gabor_response = self._apply_gabor_filters(gray)
            gabor_weight = 0.3
        else:
            gabor_response = gray  # Usa imagem original como fallback
            gabor_weight = 0.0
        
        # 3. An√°lise com Local Binary Pattern (pode ser pulada em tempo real)
        if not self.realtime_params.get('skip_lbp', False):
            lbp_image = self._apply_lbp(gray)
            lbp_variance = cv2.Laplacian(lbp_image, cv2.CV_64F).var()
            lbp_weight = 0.2
        else:
            lbp_image = gray  # Usa imagem original como fallback
            lbp_variance = 0
            lbp_weight = 0.0
        
        # 4. An√°lise de vari√¢ncia local (simplificada em tempo real)
        if not self.realtime_params.get('fast_morphology', False):
            kernel_size = 9
            kernel = np.ones((kernel_size, kernel_size)) / (kernel_size**2)
            local_mean = cv2.filter2D(gray.astype(np.float32), -1, kernel)
            local_variance = cv2.filter2D((gray.astype(np.float32) - local_mean)**2, -1, kernel)
        else:
            # Vers√£o r√°pida usando blur
            blurred = cv2.GaussianBlur(gray, (9, 9), 0)
            local_variance = cv2.absdiff(gray.astype(np.float32), blurred.astype(np.float32))
        
        # Ajusta pesos dinamicamente baseado no modo
        sobel_weight = 0.5 + gabor_weight + lbp_weight  # Compensa pesos removidos
        variance_weight = 0.5 - gabor_weight - lbp_weight
        
        # Normaliza cada componente para 0-255
        sobel_norm = cv2.normalize(sobel_magnitude, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        gabor_norm = cv2.normalize(gabor_response, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        lbp_norm = cv2.normalize(lbp_image, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        variance_norm = cv2.normalize(local_variance, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        
        # Combina texturas com pesos - garante que n√£o exceda uint8
        combined_texture = np.clip(
            sobel_norm.astype(np.float32) * sobel_weight + 
            gabor_norm.astype(np.float32) * gabor_weight +
            lbp_norm.astype(np.float32) * lbp_weight + 
            variance_norm.astype(np.float32) * variance_weight,
            0, 255
        ).astype(np.uint8)
        
        # Threshold adaptativo baseado nas caracter√≠sticas da imagem
        if self.confidence_params['adaptive_threshold'] and not self.realtime_params.get('fast_morphology', False):
            # Usa m√©todo de Otsu para threshold autom√°tico
            threshold_value, texture_mask = cv2.threshold(combined_texture, 0, 255, 
                                                        cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        else:
            # Usa threshold fixo (mais r√°pido)
            texture_mask = (combined_texture > self.texture_params['sobel_threshold']).astype(np.uint8) * 255
        
        # Opera√ß√µes morfol√≥gicas (simplificadas em tempo real)
        if self.realtime_params.get('fast_morphology', False):
            # Vers√£o r√°pida
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
            texture_mask = cv2.morphologyEx(texture_mask, cv2.MORPH_CLOSE, kernel)
        else:
            # Vers√£o completa
            kernel_small = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
            kernel_medium = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, 
                                                    (self.texture_params['morphology_kernel_size'], 
                                                     self.texture_params['morphology_kernel_size']))
            kernel_large = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9, 9))
            
            texture_mask = cv2.morphologyEx(texture_mask, cv2.MORPH_OPEN, kernel_small)
            texture_mask = cv2.morphologyEx(texture_mask, cv2.MORPH_CLOSE, kernel_medium)
            texture_mask = cv2.morphologyEx(texture_mask, cv2.MORPH_DILATE, kernel_small)
        
        # Remove componentes pequenos por √°rea
        contours, _ = cv2.findContours(texture_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        for contour in contours:
            area = cv2.contourArea(contour)
            if area < self.texture_params['min_area']:
                cv2.drawContours(texture_mask, [contour], -1, 0, -1)
        
        # Redimensiona de volta se necess√°rio
        if self.realtime_params['enabled']:
            texture_mask = self._postprocess_from_realtime(texture_mask, original_shape)
        
        # Estat√≠sticas aprimoradas
        total_pixels = original_shape[0] * original_shape[1]
        grass_pixels = np.sum(texture_mask > 0)
        coverage_percentage = (grass_pixels / total_pixels) * 100
        
        # Calcula score de confian√ßa (simplificado em tempo real)
        if self.realtime_params.get('simple_confidence', False):
            confidence = min(0.8, coverage_percentage / 50.0)  # Estimativa r√°pida
        else:
            confidence = self._calculate_confidence_score(texture_mask, enhanced_image, {'method': 'texture_based'})
        
        # An√°lise de complexidade da textura
        texture_complexity = np.std(combined_texture) / np.mean(combined_texture) if np.mean(combined_texture) > 0 else 0
        
        stats = {
            'method': 'texture_based',
            'grass_pixels': int(grass_pixels),
            'total_pixels': int(total_pixels),
            'coverage_percentage': coverage_percentage,
            'confidence_score': confidence,
            'avg_texture_score': np.mean(combined_texture),
            'max_texture_score': np.max(combined_texture),
            'texture_complexity': texture_complexity,
            'lbp_variance': lbp_variance,
            'num_components': len(contours),
            'sobel_contribution': sobel_weight,
            'gabor_contribution': gabor_weight,
            'lbp_contribution': lbp_weight,
            'variance_contribution': variance_weight
        }
        
        return texture_mask, stats
    
    def detect_grass_combined(self, image: np.ndarray) -> Tuple[np.ndarray, Dict]:
        """
        Detecta mato combinando m√∫ltiplas t√©cnicas com sistema de valida√ß√£o aprimorado.
        
        Args:
            image: Imagem de entrada
            
        Returns:
            M√°scara bin√°ria combinada e estat√≠sticas
        """
        # Pr√©-processamento baseado no modo
        if self.precision_params['enabled']:
            processed_image = self._preprocess_for_precision(image)
            logger.info("üéØ Aplicando pr√©-processamento de alta precis√£o")
        else:
            processed_image = image
        
        # Executa detec√ß√µes individuais com imagem processada
        color_mask, color_stats = self.detect_grass_color_based(processed_image)
        texture_mask, texture_stats = self.detect_grass_texture_based(processed_image)
        
        # Em modo de precis√£o, executa m√©todos adicionais
        additional_masks = []
        additional_stats = []
        
        if self.precision_params['enabled']:
            # An√°lise de gradientes direcionais
            if self.precision_params['gradient_analysis']:
                gradient_mask, gradient_stats = self._detect_gradient_patterns(processed_image)
                additional_masks.append(gradient_mask)
                additional_stats.append(gradient_stats)
            
            # An√°lise estat√≠stica avan√ßada
            if self.precision_params['statistical_features']:
                statistical_mask, statistical_stats = self._detect_statistical_patterns(processed_image)
                additional_masks.append(statistical_mask)
                additional_stats.append(statistical_stats)
        
        # Sistema de pesos adaptativos aprimorado
        weights = self._calculate_adaptive_weights([color_stats, texture_stats] + additional_stats)
        
        # Combina todas as m√°scaras
        all_masks = [color_mask, texture_mask] + additional_masks
        combined_mask = self._weighted_mask_combination(all_masks, weights)
        
        # P√≥s-processamento baseado no modo
        if self.precision_params['enabled']:
            combined_mask = self._apply_precision_postprocessing(combined_mask, processed_image)
        else:
            combined_mask = self._apply_standard_postprocessing(combined_mask)
        
        # Estat√≠sticas combinadas avan√ßadas
        stats = self._calculate_combined_stats(combined_mask, processed_image, 
                                             color_stats, texture_stats, additional_stats)
        
        return combined_mask, stats
    
    def _calculate_adaptive_weights(self, stats_list: List[Dict]) -> List[float]:
        """Calcula pesos adaptativos baseados na confian√ßa e contexto."""
        if not stats_list:
            return [1.0]
        
        # Extrai scores de confian√ßa
        confidences = []
        for stats in stats_list:
            conf = stats.get('confidence_score', 0.5)
            coverage = stats.get('coverage_percentage', 0)
            
            # Pondera confian√ßa por cobertura (mais cobertura = mais confi√°vel)
            adjusted_conf = conf * (1 + coverage / 200)  # Boost de at√© 50%
            confidences.append(adjusted_conf)
        
        # Normaliza pesos
        total_conf = sum(confidences)
        if total_conf > 0:
            weights = [conf / total_conf for conf in confidences]
        else:
            weights = [1.0 / len(stats_list)] * len(stats_list)
        
        return weights
    
    def _weighted_mask_combination(self, masks: List[np.ndarray], weights: List[float]) -> np.ndarray:
        """Combina m√°scaras com pesos espec√≠ficos."""
        if not masks:
            return np.zeros((100, 100), dtype=np.uint8)
        
        # Garante mesmo tamanho
        base_shape = masks[0].shape
        normalized_masks = []
        
        for mask in masks:
            if mask.shape != base_shape:
                resized = cv2.resize(mask, (base_shape[1], base_shape[0]))
                normalized_masks.append(resized)
            else:
                normalized_masks.append(mask)
        
        # Combina com pesos
        combined = np.zeros(base_shape, dtype=np.float32)
        
        for mask, weight in zip(normalized_masks, weights):
            combined += mask.astype(np.float32) * weight
        
        # Threshold adaptivo
        threshold = np.percentile(combined, 70) if self.precision_params['enabled'] else 127
        result = (combined > threshold).astype(np.uint8) * 255
        
        return result
    
    def _apply_precision_postprocessing(self, mask: np.ndarray, image: np.ndarray) -> np.ndarray:
        """P√≥s-processamento de alta precis√£o."""
        # Morfologia iterativa
        iterations = self.precision_params.get('morphology_iterations', 3)
        
        for i in range(iterations):
            # Kernel crescente para cada itera√ß√£o
            kernel_size = 3 + i * 2
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))
            
            # Closing seguido de opening
            mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
        
        # Watershed separation para regi√µes densas
        if self.precision_params['watershed_separation']:
            mask = self._apply_watershed_separation(mask)
        
        # Remove ru√≠do baseado em √°rea
        mask = self._remove_small_regions(mask, min_area=500)
        
        # Refina bordas usando informa√ß√£o de cor
        mask = self._refine_edges_with_color(mask, image)
        
        return mask
    
    def _apply_standard_postprocessing(self, mask: np.ndarray) -> np.ndarray:
        """P√≥s-processamento padr√£o."""
        # Morfologia b√°sica
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
        
        # Remove regi√µes muito pequenas
        mask = self._remove_small_regions(mask, min_area=1000)
        
        return mask
    
    def _detect_gradient_patterns(self, image: np.ndarray) -> Tuple[np.ndarray, Dict]:
        """Detecta padr√µes de gradiente caracter√≠sticos de vegeta√ß√£o."""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # Gradientes em m√∫ltiplas dire√ß√µes
        grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        
        magnitude = np.sqrt(grad_x**2 + grad_y**2)
        direction = np.arctan2(grad_y, grad_x) * 180 / np.pi
        
        # Padr√µes t√≠picos de vegeta√ß√£o (gradientes moderados, dire√ß√µes variadas)
        vegetation_mask = ((magnitude > 20) & (magnitude < 100)).astype(np.uint8) * 255
        
        # An√°lise de orienta√ß√£o usando gradiente da magnitude
        magnitude_normalized = (magnitude / 255.0).astype(np.float32)
        direction_variance = cv2.Laplacian(magnitude_normalized, cv2.CV_32F)
        high_variance_mask = (np.abs(direction_variance) > 0.1).astype(np.uint8) * 255
        
        # Combina crit√©rios
        gradient_mask = cv2.bitwise_and(vegetation_mask, high_variance_mask)
        
        # Estat√≠sticas
        total_pixels = gradient_mask.shape[0] * gradient_mask.shape[1]
        grass_pixels = np.sum(gradient_mask > 0)
        coverage = (grass_pixels / total_pixels) * 100
        
        stats = {
            'method': 'gradient',
            'coverage_percentage': coverage,
            'confidence_score': min(coverage / 50, 1.0),  # Mais cobertura = mais confian√ßa
            'avg_magnitude': float(np.mean(magnitude)),
            'direction_variance': float(np.mean(np.abs(direction_variance)))
        }
        
        return gradient_mask, stats
    
    def _detect_statistical_patterns(self, image: np.ndarray) -> Tuple[np.ndarray, Dict]:
        """Detecta padr√µes estat√≠sticos da vegeta√ß√£o."""
        # Converte para diferentes espa√ßos de cor
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        
        # An√°lise estat√≠stica local
        kernel_size = 15
        kernel = np.ones((kernel_size, kernel_size), np.float32) / (kernel_size**2)
        
        # M√©dia e vari√¢ncia locais em diferentes canais
        h_mean = cv2.filter2D(hsv[:,:,0].astype(np.float32), -1, kernel)
        s_var = cv2.filter2D((hsv[:,:,1].astype(np.float32) - cv2.filter2D(hsv[:,:,1].astype(np.float32), -1, kernel))**2, -1, kernel)
        
        # Crit√©rios estat√≠sticos para vegeta√ß√£o
        hue_vegetation = ((h_mean > 35) & (h_mean < 85)).astype(np.uint8)
        saturation_variation = (s_var > 200).astype(np.uint8)
        
        statistical_mask = cv2.bitwise_and(hue_vegetation * 255, saturation_variation * 255)
        
        # Morfologia para conectar regi√µes
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))
        statistical_mask = cv2.morphologyEx(statistical_mask, cv2.MORPH_CLOSE, kernel)
        
        # Estat√≠sticas
        total_pixels = statistical_mask.shape[0] * statistical_mask.shape[1]
        grass_pixels = np.sum(statistical_mask > 0)
        coverage = (grass_pixels / total_pixels) * 100
        
        stats = {
            'method': 'statistical',
            'coverage_percentage': coverage,
            'confidence_score': min(coverage / 40, 1.0),
            'avg_hue': float(np.mean(h_mean)),
            'avg_saturation_variance': float(np.mean(s_var))
        }
        
        return statistical_mask, stats
    
    def _remove_small_regions(self, mask: np.ndarray, min_area: int = 1000) -> np.ndarray:
        """Remove regi√µes menores que √°rea m√≠nima."""
        # Encontra componentes conectados
        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask, connectivity=8)
        
        # Cria nova m√°scara apenas com regi√µes grandes
        result_mask = np.zeros_like(mask)
        
        for i in range(1, num_labels):  # Pula background (label 0)
            area = stats[i, cv2.CC_STAT_AREA]
            if area >= min_area:
                result_mask[labels == i] = 255
        
        return result_mask
    
    def _refine_edges_with_color(self, mask: np.ndarray, image: np.ndarray) -> np.ndarray:
        """Refina bordas da m√°scara usando informa√ß√£o de cor."""
        # Encontra bordas da m√°scara
        edges = cv2.Canny(mask, 50, 150)
        
        # Dilata bordas para √°rea de refinamento
        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        refinement_area = cv2.dilate(edges, kernel, iterations=1)
        
        # Aplica detec√ß√£o de cor apenas na √°rea de refinamento
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        
        # M√°scara de vegeta√ß√£o baseada em cor
        green_mask = cv2.inRange(hsv, 
                                np.array([35, 40, 40]), 
                                np.array([85, 255, 255]))
        
        # Refina apenas onde h√° sobreposi√ß√£o
        refinement_mask = cv2.bitwise_and(green_mask, refinement_area)
        
        # Adiciona refinamento √† m√°scara original
        refined_mask = cv2.bitwise_or(mask, refinement_mask)
        
        return refined_mask
    
    def _calculate_combined_stats(self, mask: np.ndarray, image: np.ndarray, 
                                 color_stats: Dict, texture_stats: Dict, 
                                 additional_stats: List[Dict]) -> Dict:
        """Calcula estat√≠sticas avan√ßadas para m√©todo combinado."""
        total_pixels = mask.shape[0] * mask.shape[1]
        grass_pixels = np.sum(mask > 0)
        coverage_percentage = (grass_pixels / total_pixels) * 100
        
        # An√°lise de consenso entre m√©todos
        methods_agreement = self._analyze_methods_agreement(color_stats, texture_stats, additional_stats)
        
        # Confian√ßa final baseada em m√∫ltiplos fatores
        final_confidence = self._calculate_confidence_score(mask, image, {'method': 'combined'})
        
        # An√°lise espacial da distribui√ß√£o
        spatial_analysis = self._analyze_spatial_distribution(mask)
        
        stats = {
            'method': 'combined_enhanced' if self.precision_params['enabled'] else 'combined',
            'grass_pixels': int(grass_pixels),
            'total_pixels': int(total_pixels),
            'coverage_percentage': coverage_percentage,
            'confidence_score': final_confidence,
            'methods_agreement': methods_agreement,
            'spatial_analysis': spatial_analysis,
            'component_stats': {
                'color': color_stats,
                'texture': texture_stats,
                'additional': additional_stats
            },
            'mode': 'high_precision' if self.precision_params['enabled'] else 'standard',
            'dominant_colors': self._get_dominant_colors(image, mask)
        }
        
        mode_str = "alta precis√£o" if self.precision_params['enabled'] else "padr√£o"
        logger.info(f"Detec√ß√£o combinada ({mode_str}): {coverage_percentage:.2f}% cobertura, "
                   f"confian√ßa: {final_confidence:.3f}")
        
        return stats
    
    def _analyze_methods_agreement(self, color_stats: Dict, texture_stats: Dict, 
                                  additional_stats: List[Dict]) -> Dict:
        """Analisa concord√¢ncia entre m√©todos."""
        all_coverages = [color_stats['coverage_percentage'], texture_stats['coverage_percentage']]
        all_confidences = [color_stats.get('confidence_score', 0.5), texture_stats.get('confidence_score', 0.5)]
        
        for stats in additional_stats:
            all_coverages.append(stats['coverage_percentage'])
            all_confidences.append(stats.get('confidence_score', 0.5))
        
        return {
            'coverage_std': float(np.std(all_coverages)),
            'confidence_mean': float(np.mean(all_confidences)),
            'methods_count': len(all_coverages),
            'coverage_consistency': float(1.0 / (1.0 + np.std(all_coverages)))
        }
    
    def _analyze_spatial_distribution(self, mask: np.ndarray) -> Dict:
        """Analisa distribui√ß√£o espacial da vegeta√ß√£o detectada."""
        # Componentes conectados
        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask)
        
        if num_labels <= 1:
            return {
                'regions_count': 0,
                'avg_region_size': 0,
                'largest_region_ratio': 0,
                'fragmentation_index': 0
            }
        
        # Remove background
        areas = stats[1:, cv2.CC_STAT_AREA]
        total_vegetation_area = np.sum(areas)
        
        return {
            'regions_count': len(areas),
            'avg_region_size': float(np.mean(areas)),
            'largest_region_ratio': float(np.max(areas) / total_vegetation_area) if total_vegetation_area > 0 else 0,
            'fragmentation_index': float(len(areas) / (total_vegetation_area + 1e-6))
        }
    
    def _apply_advanced_morphology(self, mask: np.ndarray) -> np.ndarray:
        """
        Aplica sequ√™ncia aprimorada de opera√ß√µes morfol√≥gicas.
        
        Args:
            mask: M√°scara de entrada
            
        Returns:
            M√°scara processada
        """
        # Kernels de diferentes tamanhos
        kernel_small = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        kernel_medium = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        kernel_large = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))
        
        # Sequ√™ncia otimizada de opera√ß√µes
        # 1. Remove ru√≠do pequeno
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel_small)
        
        # 2. Preenche pequenos buracos
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel_medium)
        
        # 3. Suaviza bordas
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel_large)
        
        # 4. Opera√ß√£o final de abertura para separar objetos grudados
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel_small)
        
        return mask
    
    def _remove_outliers(self, mask: np.ndarray, image: np.ndarray) -> np.ndarray:
        """
        Remove componentes outliers baseado em forma e tamanho.
        
        Args:
            mask: M√°scara de entrada
            image: Imagem original
            
        Returns:
            M√°scara sem outliers
        """
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if not contours:
            return mask
        
        # Calcula estat√≠sticas dos componentes
        areas = [cv2.contourArea(c) for c in contours]
        mean_area = np.mean(areas)
        std_area = np.std(areas)
        
        # Remove componentes muito pequenos ou muito grandes
        filtered_mask = np.zeros_like(mask)
        
        for i, contour in enumerate(contours):
            area = areas[i]
            
            # Crit√©rios para manter componente:
            # 1. √Årea n√£o √© outlier (dentro de 2 desvios padr√£o)
            # 2. √Årea m√≠nima absoluta
            if (abs(area - mean_area) <= 2 * std_area and 
                area >= self.texture_params['min_area']):
                
                # Teste adicional: raz√£o aspecto
                x, y, w, h = cv2.boundingRect(contour)
                aspect_ratio = w / h if h > 0 else 0
                
                # Vegeta√ß√£o geralmente n√£o tem raz√µes de aspecto extremas
                if 0.1 <= aspect_ratio <= 10:
                    cv2.drawContours(filtered_mask, [contour], -1, 255, -1)
        
        return filtered_mask
    
    def _detect_problematic_scenarios(self, image: np.ndarray, mask: np.ndarray,
                                    color_stats: Dict, texture_stats: Dict) -> Dict:
        """
        Detecta cen√°rios que podem afetar a confiabilidade da detec√ß√£o.
        
        Args:
            image: Imagem original
            mask: M√°scara de detec√ß√£o
            color_stats: Estat√≠sticas do m√©todo de cor
            texture_stats: Estat√≠sticas do m√©todo de textura
            
        Returns:
            Dicion√°rio com flags de cen√°rios problem√°ticos
        """
        flags = {}
        
        # 1. Ilumina√ß√£o problem√°tica
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        brightness = np.mean(gray) / 255.0
        flags['low_light'] = brightness < 0.2
        flags['overexposed'] = brightness > 0.9
        
        # 2. Contraste baixo
        contrast = np.std(gray) / 255.0
        flags['low_contrast'] = contrast < 0.1
        
        # 3. Discord√¢ncia entre m√©todos
        color_coverage = color_stats.get('coverage_percentage', 0)
        texture_coverage = texture_stats.get('coverage_percentage', 0)
        
        if max(color_coverage, texture_coverage) > 0:
            disagreement = abs(color_coverage - texture_coverage) / max(color_coverage, texture_coverage)
            flags['method_disagreement'] = disagreement > 0.5
        else:
            flags['method_disagreement'] = False
        
        # 4. Detec√ß√£o muito esparsa ou muito densa
        coverage = np.sum(mask > 0) / (mask.shape[0] * mask.shape[1])
        flags['sparse_detection'] = coverage < 0.05
        flags['dense_detection'] = coverage > 0.8
        
        # 5. Qualidade de foco
        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
        flags['poor_focus'] = laplacian_var < 100
        
        return flags
    
    def detect_grass_areas(self, image: np.ndarray, method: str = 'combined') -> Tuple[np.ndarray, Dict]:
        """
        Detecta √°reas com mato alto na imagem.
        
        Args:
            image: Imagem de entrada
            method: M√©todo de detec√ß√£o ('color', 'texture', 'combined', 'deeplearning')
            
        Returns:
            M√°scara bin√°ria das √°reas detectadas e estat√≠sticas
        """
        if image is None:
            raise ValueError("Imagem n√£o pode ser None")
        
        logger.info(f"Detectando mato usando m√©todo: {method}")
        
        # Redimensiona se muito grande
        height, width = image.shape[:2]
        if width > 1920 or height > 1080:
            scale = min(1920/width, 1080/height)
            new_width = int(width * scale)
            new_height = int(height * scale)
            image = cv2.resize(image, (new_width, new_height))
            logger.info(f"Imagem redimensionada para: {new_width}x{new_height}")
        
        if method == 'color':
            return self.detect_grass_color_based(image)
        elif method == 'texture':
            return self.detect_grass_texture_based(image)
        elif method == 'combined':
            return self.detect_grass_combined(image)
        elif method == 'deeplearning' and self.is_model_loaded and HAS_TENSORFLOW:
            return self.detect_grass_deep_learning(image)
        else:
            # Fallback para m√©todo combinado
            return self.detect_grass_combined(image)
    
    def detect_grass_deep_learning(self, image: np.ndarray) -> Tuple[np.ndarray, Dict]:
        """
        Detecta mato usando modelo de deep learning aprimorado.
        
        Args:
            image: Imagem de entrada
            
        Returns:
            M√°scara e estat√≠sticas
        """
        if not HAS_TENSORFLOW:
            logger.warning("TensorFlow n√£o est√° dispon√≠vel, usando m√©todo combinado")
            return self.detect_grass_combined(image)
        
        if not self.is_model_loaded or self.model is None:
            logger.warning("Modelo n√£o carregado, usando m√©todo combinado")
            return self.detect_grass_combined(image)
        
        try:
            original_shape = image.shape[:2]
            
            # Pr√©-processamento aprimorado
            if self.precision_params['enabled']:
                preprocessed = self._preprocess_for_precision(image)
            else:
                preprocessed = image
            
            # M√∫ltiplas escalas para robustez
            scales = [1.0]
            if self.precision_params['enabled'] and self.precision_params['multi_scale_analysis']:
                scales = [0.75, 1.0, 1.25]  # Multi-escala restaurado com corre√ß√µes
            
            predictions = []
            
            for scale in scales:
                # Redimensiona para a escala
                if scale != 1.0:
                    new_height = int(original_shape[0] * scale)
                    new_width = int(original_shape[1] * scale)
                    scaled_image = cv2.resize(preprocessed, (new_width, new_height))
                else:
                    scaled_image = preprocessed.copy()
                
                # Prepara para o modelo
                target_height = ((scaled_image.shape[0] // 32) + 1) * 32
                target_width = ((scaled_image.shape[1] // 32) + 1) * 32
                
                # Padding para m√∫ltiplos de 32
                padded = cv2.copyMakeBorder(
                    scaled_image, 0, target_height - scaled_image.shape[0],
                    0, target_width - scaled_image.shape[1], 
                    cv2.BORDER_REFLECT
                )
                
                # Normaliza√ß√£o avan√ßada
                normalized = padded.astype(np.float32) / 255.0
                
                # Adiciona channel de textura se habilitado
                if self.precision_params['enabled'] and self.precision_params['texture_coherence']:
                    gray = cv2.cvtColor(padded, cv2.COLOR_BGR2GRAY)
                    texture_channel = cv2.Laplacian(gray, cv2.CV_64F) / 255.0
                    texture_channel = np.abs(texture_channel).astype(np.float32)
                    texture_channel = np.expand_dims(texture_channel, axis=2)
                    
                    # Concatena canal de textura
                    normalized = np.concatenate([normalized, texture_channel], axis=2)
                    
                    # Ajusta input do modelo para aceitar 4 canais
                    if normalized.shape[2] == 4:
                        # Se modelo n√£o suporta 4 canais, usa apenas RGB
                        normalized = normalized[:, :, :3]
                
                # Batch dimension
                input_batch = np.expand_dims(normalized, axis=0)
                
                logger.info(f"Executando predi√ß√£o deep learning (escala {scale:.2f})...")
                
                # Predi√ß√£o com modelo aprimorado
                if hasattr(self.model, 'predict'):
                    # Modelo real treinado
                    outputs = self.model.predict(input_batch, verbose=0)
                    
                    if isinstance(outputs, list):
                        # Modelo com m√∫ltiplas sa√≠das
                        segmentation = outputs[0][0]
                        density = outputs[1][0] if len(outputs) > 1 else None
                        confidence = outputs[2][0] if len(outputs) > 2 else None
                    else:
                        segmentation = outputs[0]
                        density = None
                        confidence = None
                else:
                    # Simula√ß√£o aprimorada para demonstra√ß√£o
                    segmentation, density, confidence = self._advanced_deep_learning_simulation(normalized)
                
                # Remove padding
                segmentation = segmentation[:scaled_image.shape[0], :scaled_image.shape[1]]
                
                # Redimensiona de volta para escala original
                if scale != 1.0:
                    segmentation = cv2.resize(segmentation, (original_shape[1], original_shape[0]))
                
                predictions.append({
                    'segmentation': segmentation,
                    'density': density,
                    'confidence': confidence,
                    'scale': scale
                })
            
            # Ensemble das predi√ß√µes multi-escala
            final_prediction = self._ensemble_predictions(predictions)
            
            # P√≥s-processamento avan√ßado
            mask = self._postprocess_deep_learning(final_prediction, original_shape)
            
            # Estat√≠sticas aprimoradas
            stats = self._calculate_deep_learning_stats(mask, final_prediction, image)
            
            return mask, stats
            
        except Exception as e:
            logger.error(f"Erro na predi√ß√£o deep learning: {str(e)}")
            logger.info("Fallback para m√©todo combinado")
            return self.detect_grass_combined(image)
    
    def _advanced_deep_learning_simulation(self, normalized_image: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """
        Simula√ß√£o avan√ßada de deep learning com an√°lise rigorosa para vegeta√ß√£o.
        """
        # An√°lise multiespectral mais rigorosa
        vegetation_score = self._extract_vegetation_specific_features(normalized_image)
        exclusion_mask = self._extract_non_vegetation_features(normalized_image)
        context_features = self._extract_contextual_features(normalized_image)
        
        # Combina com pesos balanceados e exclus√£o rigorosa
        combined_features = vegetation_score * context_features
        
        # Remove explicitamente √°reas que N√ÉO s√£o vegeta√ß√£o
        combined_features = combined_features * (1.0 - exclusion_mask)
        
        # Aplicar threshold mais rigoroso (vegeta√ß√£o precisa ser bem caracterizada)
        segmentation = combined_features.copy()
        
        # Filtro de suaviza√ß√£o preservando bordas (bilateral)
        if len(segmentation.shape) == 2:
            # Converte para 8-bit para bilateral filter
            seg_8bit = (segmentation * 255).astype(np.uint8)
            bilateral = cv2.bilateralFilter(seg_8bit, 9, 75, 75)
            segmentation = bilateral.astype(np.float32) / 255.0
        
        # Fun√ß√£o de ativa√ß√£o mais conservadora (sigmoid balanceado)
        segmentation = 1.0 / (1.0 + np.exp(-8 * (segmentation - 0.4)))  # Threshold moderado
        
        # Densidade baseada em an√°lise local de vegeta√ß√£o
        density = self._calculate_vegetation_density(segmentation, normalized_image)
        
        # Confian√ßa baseada em m√∫ltiplos crit√©rios
        confidence = self._calculate_advanced_prediction_confidence(segmentation, normalized_image)
        
        return segmentation, density, confidence
    
    def _extract_vegetation_specific_features(self, image: np.ndarray) -> np.ndarray:
        """Extrai caracter√≠sticas espec√≠ficas de vegeta√ß√£o com alta precis√£o."""
        if len(image.shape) == 3:
            rgb_image = (image * 255).astype(np.uint8)
        else:
            rgb_image = cv2.cvtColor((image * 255).astype(np.uint8), cv2.COLOR_GRAY2BGR)
        
        # M√∫ltiplos espa√ßos de cor para an√°lise precisa
        hsv = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2HSV).astype(np.float32) / 255.0
        lab = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2LAB).astype(np.float32) / 255.0
        rgb_norm = rgb_image.astype(np.float32) / 255.0
        
        vegetation_scores = []
        
        # 1. An√°lise HSV mais rigorosa para vegeta√ß√£o
        h, s, v = hsv[:, :, 0], hsv[:, :, 1], hsv[:, :, 2]
        
        # Verde natural (mais restritivo)
        green_natural = ((h > 0.28) & (h < 0.42) &     # Faixa verde espec√≠fica
                        (s > 0.4) & (s < 0.9) &         # Satura√ß√£o moderada/alta
                        (v > 0.25) & (v < 0.85))        # N√£o muito escuro/claro
        
        # Verde de gram√≠nea (tom espec√≠fico)
        grass_green = ((h > 0.25) & (h < 0.35) &       # Verde amarelado t√≠pico
                      (s > 0.3) & (s < 0.7) &          # Satura√ß√£o moderada
                      (v > 0.3) & (v < 0.7))           # Luminosidade m√©dia
        
        # Verde de folhagem (tom espec√≠fico)
        foliage_green = ((h > 0.35) & (h < 0.48) &     # Verde mais puro
                        (s > 0.5) & (s < 0.95) &        # Alta satura√ß√£o
                        (v > 0.2) & (v < 0.8))          # N√£o extremos
        
        # Combina detectores de verde
        green_score = (green_natural.astype(float) * 0.4 + 
                      grass_green.astype(float) * 0.4 + 
                      foliage_green.astype(float) * 0.3)
        
        # Pondera pela qualidade da cor (evita tons artificiais)
        color_quality = np.minimum(s, 1.0 - np.abs(v - 0.5))  # Prefer moderate brightness, high saturation
        green_score *= color_quality
        
        vegetation_scores.append(green_score)
        
        # 2. An√°lise LAB para detectar clorofila
        a_channel = lab[:, :, 1] - 0.5  # Canal A centrado
        b_channel = lab[:, :, 2] - 0.5  # Canal B centrado
        
        # Vegeta√ß√£o tem A negativo (verde) e B ligeiramente positivo
        chlorophyll_score = np.where(
            (a_channel < -0.02) & (b_channel > -0.1) & (b_channel < 0.15),
            np.abs(a_channel) * 2,  # Intensifica sinal verde
            0.0
        )
        vegetation_scores.append(chlorophyll_score)
        
        # 3. An√°lise de √≠ndices espectrais simulados
        r, g, b = rgb_norm[:, :, 2], rgb_norm[:, :, 1], rgb_norm[:, :, 0]
        
        # NDVI simulado (Normalized Difference Vegetation Index)
        # Normalmente seria (NIR - Red) / (NIR + Red), simulamos com green
        ndvi_sim = np.where(
            (g + r) > 0,
            (g - r) / (g + r + 1e-6),
            0
        )
        ndvi_score = np.where(ndvi_sim > 0.1, ndvi_sim, 0)  # NDVI positivo indica vegeta√ß√£o
        vegetation_scores.append(ndvi_score)
        
        # 4. Excess Green Index
        eg_index = 2 * g - r - b
        eg_score = np.where(eg_index > 0.05, eg_index, 0)  # Verde predominante
        vegetation_scores.append(eg_score)
        
        # Combina todos os scores com pesos otimizados
        final_score = (vegetation_scores[0] * 0.35 +     # HSV verde
                      vegetation_scores[1] * 0.25 +      # Clorofila (LAB)
                      vegetation_scores[2] * 0.25 +      # NDVI simulado
                      vegetation_scores[3] * 0.15)       # Excess Green
        
        return np.clip(final_score, 0, 1)
    
    def _extract_non_vegetation_features(self, image: np.ndarray) -> np.ndarray:
        """Identifica √°reas que definitivamente N√ÉO s√£o vegeta√ß√£o."""
        if len(image.shape) == 3:
            rgb_image = (image * 255).astype(np.uint8)
        else:
            rgb_image = cv2.cvtColor((image * 255).astype(np.uint8), cv2.COLOR_GRAY2BGR)
        
        hsv = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2HSV).astype(np.float32) / 255.0
        rgb_norm = rgb_image.astype(np.float32) / 255.0
        
        exclusion_masks = []
        
        # 1. Objetos artificiais (cores n√£o naturais)
        h, s, v = hsv[:, :, 0], hsv[:, :, 1], hsv[:, :, 2]
        
        # Cores altamente saturadas n√£o-verdes (objetos artificiais)
        artificial_colors = (s > 0.8) & ((h < 0.2) | (h > 0.5))
        exclusion_masks.append(artificial_colors.astype(float))
        
        # 2. Superf√≠cies muito claras (c√©u, papel, etc.)
        bright_surfaces = v > 0.9
        exclusion_masks.append(bright_surfaces.astype(float))
        
        # 3. Superf√≠cies muito escuras (sombras profundas, objetos escuros)
        dark_surfaces = v < 0.15
        exclusion_masks.append(dark_surfaces.astype(float))
        
        # 4. Tons de cinza (baixa satura√ß√£o) - geralmente n√£o √© vegeta√ß√£o vibrante
        gray_tones = s < 0.15
        exclusion_masks.append(gray_tones.astype(float) * 0.7)  # Peso menor
        
        # 5. An√°lise de textura - superf√≠cies muito lisas (vidro, metal, etc.)
        gray = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2GRAY)
        laplacian = cv2.Laplacian(gray, cv2.CV_64F)
        texture_variance = cv2.GaussianBlur(np.abs(laplacian), (15, 15), 0)
        
        smooth_surfaces = texture_variance < 20  # Muito liso
        exclusion_masks.append(smooth_surfaces.astype(float) * 0.5)
        
        # 6. Detec√ß√£o de pele humana (tons espec√≠ficos)
        r, g, b = rgb_norm[:, :, 2], rgb_norm[:, :, 1], rgb_norm[:, :, 0]
        
        # Crit√©rios b√°sicos para tons de pele
        skin_mask = ((r > g) & (g > b) &                    # R > G > B
                    (r > 0.4) & (r < 0.9) &                 # Faixa do vermelho
                    ((r - g) > 0.05) &                      # Diferen√ßa R-G
                    ((g - b) > 0.02))                       # Diferen√ßa G-B
        exclusion_masks.append(skin_mask.astype(float))
        
        # Combina todas as m√°scaras de exclus√£o
        final_exclusion = np.zeros_like(exclusion_masks[0])
        
        for mask in exclusion_masks:
            final_exclusion = np.maximum(final_exclusion, mask)
        
        return np.clip(final_exclusion, 0, 1)
    
    def _extract_contextual_features(self, image: np.ndarray) -> np.ndarray:
        """Extrai caracter√≠sticas contextuais que refor√ßam a presen√ßa de vegeta√ß√£o."""
        if len(image.shape) == 3:
            rgb_image = (image * 255).astype(np.uint8)
        else:
            rgb_image = cv2.cvtColor((image * 255).astype(np.uint8), cv2.COLOR_GRAY2BGR)
        
        gray = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2GRAY)
        
        # 1. An√°lise de textura natural (vegeta√ß√£o tem textura espec√≠fica)
        # Filtros Gabor para detectar padr√µes de vegeta√ß√£o
        gabor_responses = []
        
        # M√∫ltiplos filtros para capturar texturas de folhas/grama
        for angle in [0, 30, 60, 90, 120, 150]:  # Mais √¢ngulos
            for freq in [0.1, 0.2, 0.3]:
                kernel = cv2.getGaborKernel((21, 21), 4, np.radians(angle), 
                                          2 * np.pi * freq, 0.5, 0, ktype=cv2.CV_32F)
                response = cv2.filter2D(gray, cv2.CV_8UC3, kernel)
                gabor_responses.append(np.abs(response.astype(float)))
        
        # Combina respostas Gabor (vegeta√ß√£o tem resposta moderada e variada)
        gabor_combined = np.mean(gabor_responses, axis=0) / 255.0
        texture_score = np.where(
            (gabor_combined > 0.1) & (gabor_combined < 0.6),  # Textura moderada
            gabor_combined,
            gabor_combined * 0.3  # Penaliza texturas extremas
        )
        
        # 2. An√°lise de gradientes locais (vegeta√ß√£o tem gradientes suaves)
        grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        gradient_mag = np.sqrt(grad_x**2 + grad_y**2) / 255.0
        
        # Vegeta√ß√£o tem gradientes moderados (n√£o bordas muito duras)
        gradient_score = np.where(
            (gradient_mag > 0.05) & (gradient_mag < 0.4),
            gradient_mag * 2,
            gradient_mag * 0.5
        )
        
        # 3. Consist√™ncia local (vegeta√ß√£o tende a aparecer em grupos)
        kernel_consistency = np.ones((11, 11)) / 121
        local_variance = cv2.filter2D(gray.astype(np.float32), -1, kernel_consistency)
        local_std = np.sqrt(np.abs(cv2.filter2D((gray.astype(np.float32) - local_variance)**2, -1, kernel_consistency)))
        
        consistency_score = 1.0 / (1.0 + local_std / 50.0)  # Menor varia√ß√£o = maior score
        
        # Combina caracter√≠sticas contextuais
        context_features = (texture_score * 0.4 + 
                          gradient_score * 0.35 + 
                          consistency_score * 0.25)
        
        return np.clip(context_features, 0, 1)
    
    def _calculate_vegetation_density(self, segmentation: np.ndarray, original_image: np.ndarray) -> np.ndarray:
        """Calcula densidade de vegeta√ß√£o baseada em an√°lise local."""
        # Densidade baseada na concentra√ß√£o local de vegeta√ß√£o
        density_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (25, 25))
        density_kernel = density_kernel.astype(np.float32) / np.sum(density_kernel)
        
        # Aplica filtro de densidade
        local_density = cv2.filter2D(segmentation, -1, density_kernel)
        
        # Ajusta densidade baseado na textura local
        if len(original_image.shape) == 3:
            gray = cv2.cvtColor((original_image * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)
        else:
            gray = (original_image * 255).astype(np.uint8)
        
        # Textura local como fator de densidade
        texture_factor = cv2.Laplacian(gray, cv2.CV_64F)
        texture_normalized = np.abs(texture_factor) / 255.0
        texture_factor = np.clip(texture_normalized, 0, 1)
        
        # Combina densidade espacial com textura
        final_density = local_density * (0.7 + 0.3 * texture_factor)
        
        return np.clip(final_density, 0, 1)
    
    def _calculate_advanced_prediction_confidence(self, segmentation: np.ndarray, original_image: np.ndarray) -> np.ndarray:
        """Calcula confian√ßa avan√ßada baseada em m√∫ltiplos crit√©rios."""
        confidences = []
        
        # 1. Confian√ßa baseada na certeza da predi√ß√£o (dist√¢ncia de 0.5)
        prediction_certainty = 2 * np.abs(segmentation - 0.5)
        confidences.append(prediction_certainty)
        
        # 2. Confian√ßa baseada na consist√™ncia local
        kernel = np.ones((9, 9)) / 81
        local_mean = cv2.filter2D(segmentation, -1, kernel)
        local_variance = cv2.filter2D((segmentation - local_mean)**2, -1, kernel)
        consistency_conf = 1.0 / (1.0 + local_variance * 10)
        confidences.append(consistency_conf)
        
        # 3. Confian√ßa baseada na qualidade da cor
        if len(original_image.shape) == 3:
            rgb_image = (original_image * 255).astype(np.uint8)
            hsv = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2HSV).astype(np.float32) / 255.0
            
            # Confian√ßa alta para cores t√≠picas de vegeta√ß√£o
            h, s, v = hsv[:, :, 0], hsv[:, :, 1], hsv[:, :, 2]
            color_conf = np.where(
                (h > 0.25) & (h < 0.5) & (s > 0.3) & (v > 0.2) & (v < 0.8),
                0.8 + 0.2 * s,  # Maior confian√ßa para vegeta√ß√£o bem saturada
                0.3             # Baixa confian√ßa para outras cores
            )
            confidences.append(color_conf)
        
        # 4. Confian√ßa baseada na textura adequada
        if len(original_image.shape) == 3:
            gray = cv2.cvtColor((original_image * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)
        else:
            gray = (original_image * 255).astype(np.uint8)
        
        # Analisa textura local
        laplacian = cv2.Laplacian(gray, cv2.CV_64F)
        texture_strength = np.abs(laplacian) / 255.0
        
        # Confian√ßa moderada para texturas t√≠picas de vegeta√ß√£o
        texture_conf = np.where(
            (texture_strength > 0.1) & (texture_strength < 0.5),
            0.7 + 0.3 * texture_strength,
            0.4  # Menor confian√ßa para texturas extremas
        )
        confidences.append(texture_conf)
        
        # Combina todas as confid√™ncias com verifica√ß√£o de dimens√µes
        aligned_confidences = []
        target_shape = segmentation.shape[:2]  # Garante 2D
        
        for conf in confidences:
            if len(conf.shape) == 3:
                conf = conf[:, :, 0]  # Pega primeiro canal se 3D
            if conf.shape != target_shape:
                conf = cv2.resize(conf, (target_shape[1], target_shape[0]))
            aligned_confidences.append(conf)
        
        final_confidence = np.mean(aligned_confidences, axis=0)
        
        # NOVA: Aplica boost adaptativo baseado no aprendizado
        try:
            adaptive_boost = self.adaptive_learning.get_adaptive_confidence_boost(segmentation, original_image)
            logger.debug(f"Boost adaptativo aplicado: {adaptive_boost:.3f}")
            
            # Aplica boost de forma inteligente
            boosted_confidence = final_confidence * adaptive_boost
            
            # Garante que n√£o ultrapasse limites razo√°veis
            final_confidence = np.clip(boosted_confidence, 0, 1)
            
        except Exception as e:
            logger.debug(f"Erro ao aplicar boost adaptativo: {e}")
            # Usa confian√ßa original se houver erro
        
        return np.clip(final_confidence, 0, 1)
        
        return np.clip(final_confidence, 0, 1)
    
    def _extract_rgb_features(self, image: np.ndarray) -> np.ndarray:
        """Extrai caracter√≠sticas RGB aprimoradas."""
        if len(image.shape) == 3:
            rgb_image = (image * 255).astype(np.uint8)
        else:
            rgb_image = cv2.cvtColor((image * 255).astype(np.uint8), cv2.COLOR_GRAY2BGR)
        
        # Converte para HSV para an√°lise de cor
        hsv = cv2.cvtColor(rgb_image, cv2.COLOR_BGR2HSV).astype(np.float32) / 255.0
        
        # M√∫ltiplas faixas de verde
        green_masks = []
        
        # Verde vibrante
        mask1 = ((hsv[:, :, 0] > 0.25) & (hsv[:, :, 0] < 0.45) & 
                (hsv[:, :, 1] > 0.3) & (hsv[:, :, 2] > 0.2))
        
        # Verde escuro
        mask2 = ((hsv[:, :, 0] > 0.3) & (hsv[:, :, 0] < 0.5) & 
                (hsv[:, :, 1] > 0.4) & (hsv[:, :, 2] > 0.1) & (hsv[:, :, 2] < 0.6))
        
        # Verde amarelado
        mask3 = ((hsv[:, :, 0] > 0.15) & (hsv[:, :, 0] < 0.35) & 
                (hsv[:, :, 1] > 0.25) & (hsv[:, :, 2] > 0.25))
        
        # Combina m√°scaras com pesos
        color_score = (mask1.astype(float) * 0.6 + 
                      mask2.astype(float) * 0.8 + 
                      mask3.astype(float) * 0.4)
        
        # Pondera pela satura√ß√£o e valor
        color_score *= (hsv[:, :, 1] * hsv[:, :, 2])
        
        return np.clip(color_score, 0, 1)
    
    def _extract_advanced_texture_features(self, image: np.ndarray) -> np.ndarray:
        """Extrai caracter√≠sticas de textura avan√ßadas."""
        if len(image.shape) == 3:
            gray = cv2.cvtColor((image * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)
        else:
            gray = (image * 255).astype(np.uint8)
        
        # M√∫ltiplos filtros de textura
        texture_responses = []
        
        # Filtros Gabor multi-orienta√ß√£o
        gabor_angles = [0, 45, 90, 135]
        gabor_frequencies = [0.1, 0.2, 0.3]
        
        for angle in gabor_angles:
            for freq in gabor_frequencies:
                kernel = cv2.getGaborKernel((21, 21), 5, np.radians(angle), 
                                          2 * np.pi * freq, 0.5, 0, ktype=cv2.CV_32F)
                response = cv2.filter2D(gray, cv2.CV_8UC3, kernel)
                texture_responses.append(response.astype(float) / 255.0)
        
        # LBP (Local Binary Pattern)
        lbp = self._compute_lbp(gray, 8, 1)
        texture_responses.append(lbp.astype(float) / 255.0)
        
        # Combine responses
        combined_texture = np.mean(texture_responses, axis=0)
        
        # Normaliza para vegeta√ß√£o t√≠pica
        texture_score = np.where(
            (combined_texture > 0.2) & (combined_texture < 0.7),
            combined_texture,
            combined_texture * 0.5
        )
        
        return texture_score
    
    def _extract_spatial_features(self, image: np.ndarray) -> np.ndarray:
        """Extrai caracter√≠sticas espaciais."""
        if len(image.shape) == 3:
            gray = cv2.cvtColor((image * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)
        else:
            gray = (image * 255).astype(np.uint8)
        
        # Gradientes direcionais
        grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        
        magnitude = np.sqrt(grad_x**2 + grad_y**2) / 255.0
        
        # Padr√µes espaciais t√≠picos de vegeta√ß√£o
        spatial_score = np.where(
            (magnitude > 0.15) & (magnitude < 0.6),
            magnitude * 1.2,
            magnitude * 0.6
        )
        
        return np.clip(spatial_score, 0, 1)
    
    def _calculate_prediction_confidence(self, segmentation: np.ndarray) -> np.ndarray:
        """Calcula mapa de confian√ßa da predi√ß√£o."""
        # Vari√¢ncia local como medida de incerteza
        kernel = np.ones((7, 7)) / 49
        local_mean = cv2.filter2D(segmentation, -1, kernel)
        local_variance = cv2.filter2D(segmentation**2, -1, kernel) - local_mean**2
        
        # Confian√ßa inversa √† vari√¢ncia
        confidence = 1.0 - np.clip(local_variance * 10, 0, 1)
        
        # Aumenta confian√ßa em predi√ß√µes extremas (pr√≥ximas de 0 ou 1)
        extreme_confidence = 2 * np.abs(segmentation - 0.5)
        
        return np.maximum(confidence, extreme_confidence)
    
    def _ensemble_predictions(self, predictions: List[Dict]) -> np.ndarray:
        """Combina predi√ß√µes de m√∫ltiplas escalas."""
        if len(predictions) == 1:
            return predictions[0]['segmentation']
        
        # M√©dia ponderada baseada na confian√ßa
        weighted_sum = np.zeros_like(predictions[0]['segmentation'])
        weight_sum = 0
        
        for pred in predictions:
            if pred['confidence'] is not None:
                # Garante que confidence tem as dimens√µes corretas
                confidence = pred['confidence']
                if isinstance(confidence, np.ndarray):
                    if confidence.shape == pred['segmentation'].shape:
                        weight = np.mean(confidence)
                    else:
                        # Se as dimens√µes n√£o coincidem, usa valor escalar
                        weight = float(np.mean(confidence))
                else:
                    weight = float(confidence)
            else:
                weight = 1.0
            
            weighted_sum += pred['segmentation'] * weight
            weight_sum += weight
        
        return weighted_sum / weight_sum if weight_sum > 0 else weighted_sum
    
    def _postprocess_deep_learning(self, prediction: np.ndarray, original_shape: tuple) -> np.ndarray:
        """P√≥s-processamento rigoroso para deep learning com alta precis√£o."""
        
        # Threshold mais conservador e adaptativo
        # Usa m√∫ltiplos percentis para encontrar threshold √≥timo
        p50 = np.percentile(prediction, 50)  # Mediana
        p75 = np.percentile(prediction, 75)  # 75¬∫ percentil
        p85 = np.percentile(prediction, 85)  # 85¬∫ percentil
        
        # Escolhe threshold baseado na distribui√ß√£o (menos restritivo)
        if p75 > 0.3:  # Se 75% das predi√ß√µes s√£o razo√°veis
            threshold = max(0.4, p50)  # Usa mediana ou m√≠nimo de 0.4
        elif p50 > 0.2:  # Distribui√ß√£o moderada
            threshold = max(0.3, p50)  
        else:  # Distribui√ß√£o baixa - ainda conservador mas detect√°vel
            threshold = max(0.2, p75)
        
        threshold = min(threshold, 0.6)  # Limita m√°ximo para manter precis√£o
        
        logger.info(f"üéØ Deep learning usando threshold rigoroso: {threshold:.3f}")
        
        # Aplica threshold
        mask = (prediction > threshold).astype(np.uint8) * 255
        
        # Morfologia rigorosa para remover falsos positivos
        # Primeiro remove ru√≠do pequeno
        kernel_noise = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel_noise, iterations=2)
        
        # Conecta regi√µes pr√≥ximas de vegeta√ß√£o  
        kernel_connect = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))
        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel_connect, iterations=1)
        
        # Remove componentes muito pequenos (provavelmente falsos positivos)
        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask, connectivity=8)
        
        min_area = 100  # √Årea m√≠nima para considerir como vegeta√ß√£o
        if self.precision_params['enabled']:
            min_area = 200  # Mais rigoroso em modo precis√£o
        
        # Cria m√°scara limpa
        clean_mask = np.zeros_like(mask)
        
        for i in range(1, num_labels):  # Pula background (0)
            area = stats[i, cv2.CC_STAT_AREA]
            if area >= min_area:
                clean_mask[labels == i] = 255
        
        # Valida√ß√£o adicional por cor se em modo precis√£o
        if self.precision_params['enabled']:
            clean_mask = self._validate_deep_learning_by_color(clean_mask, original_shape, prediction)
        
        # Suaviza√ß√£o final das bordas
        kernel_smooth = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
        clean_mask = cv2.morphologyEx(clean_mask, cv2.MORPH_CLOSE, kernel_smooth)
        
        return clean_mask
    
    def _validate_deep_learning_by_color(self, mask: np.ndarray, original_shape: tuple, prediction: np.ndarray) -> np.ndarray:
        """Valida√ß√£o adicional das detec√ß√µes deep learning atrav√©s de an√°lise de cor."""
        # Esta fun√ß√£o seria chamada apenas em modo de alta precis√£o
        # Reconstr√≥i an√°lise de cor para validar as detec√ß√µes
        
        # Para implementa√ß√£o futura: poderia usar a imagem original para validar
        # se as √°reas detectadas realmente t√™m cores de vegeta√ß√£o
        
        # Garante que prediction tem as dimens√µes corretas
        if len(prediction.shape) == 3:
            prediction = prediction[:, :, 0]  # Pega apenas o primeiro canal se for 3D
        
        # Por enquanto, aplica filtro conservador baseado na confian√ßa da predi√ß√£o
        high_confidence_mask = (prediction > 0.8).astype(np.uint8) * 255
        
        # Mant√©m apenas regi√µes de alta confian√ßa ou que j√° passaram pelos filtros b√°sicos
        validated_mask = cv2.bitwise_and(mask, high_confidence_mask)
        
        # Se perdemos muita √°rea, mant√©m uma vers√£o menos restritiva
        original_area = np.sum(mask > 0)
        validated_area = np.sum(validated_mask > 0)
        
        if validated_area < original_area * 0.3:  # Perdemos mais de 70%
            # Usa crit√©rio menos rigoroso
            medium_confidence_mask = (prediction > 0.6).astype(np.uint8) * 255
            validated_mask = cv2.bitwise_and(mask, medium_confidence_mask)
        
        return validated_mask
    
    def _calculate_deep_learning_stats(self, mask: np.ndarray, prediction: np.ndarray, original_image: np.ndarray) -> Dict:
        """Calcula estat√≠sticas aprimoradas para deep learning."""
        total_pixels = mask.shape[0] * mask.shape[1]
        grass_pixels = np.sum(mask > 0)
        coverage_percentage = (grass_pixels / total_pixels) * 100
        
        # Confian√ßa baseada na certeza das predi√ß√µes
        prediction_certainty = np.mean(2 * np.abs(prediction - 0.5))
        
        # NOVO: Sistema de valida√ß√£o por consenso - testa com m√∫ltiplos m√©todos
        consensus_confidence = self._calculate_consensus_confidence(mask, original_image)
        
        # Combina confian√ßa da predi√ß√£o com consenso
        combined_certainty = (prediction_certainty * 0.6 + consensus_confidence * 0.4)
        
        # NOVO: Aplica par√¢metros adaptativos aprendidos
        try:
            adaptive_params = self.adaptive_learning.get_adaptive_parameters()
            adaptive_boost = adaptive_params['confidence_boost']
            combined_certainty *= adaptive_boost
            logger.debug(f"Confian√ßa aprimorada com boost adaptativo: {adaptive_boost:.3f}")
        except Exception as e:
            logger.debug(f"Usando confian√ßa padr√£o: {e}")
        
        # Limita a faixa de confian√ßa
        prediction_certainty = np.clip(combined_certainty, 0.05, 0.95)
        
        # An√°lise de distribui√ß√£o espacial
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        regions_count = len(contours)
        if contours:
            areas = [cv2.contourArea(c) for c in contours]
            avg_region_size = np.mean(areas) if areas else 0
            max_region_size = np.max(areas) if areas else 0
            fragmentation = regions_count / (coverage_percentage + 1e-6)
        else:
            avg_region_size = 0
            max_region_size = 0
            fragmentation = 0
        
        stats = {
            'method': 'deeplearning_enhanced',
            'grass_pixels': int(grass_pixels),
            'total_pixels': int(total_pixels),
            'coverage_percentage': coverage_percentage,
            'confidence_score': prediction_certainty,
            'prediction_stats': {
                'mean': float(np.mean(prediction)),
                'std': float(np.std(prediction)),
                'min': float(np.min(prediction)),
                'max': float(np.max(prediction)),
                'threshold_used': float(np.percentile(prediction, 60))
            },
            'spatial_analysis': {
                'regions_count': regions_count,
                'avg_region_size': float(avg_region_size),
                'max_region_size': float(max_region_size),
                'fragmentation_index': float(fragmentation)
            },
            'dominant_colors': self._get_dominant_colors(original_image, mask),
            'learning_stats': {
                'adaptive_boost_applied': True,
                'examples_learned': self.adaptive_learning.stats['examples_learned'],
                'confidence_boost_factor': self.adaptive_learning.adaptive_params.get('confidence_boost', 1.0),
                'consensus_validation': True
            }
        }
        
        logger.info(f"Deep learning enhanced: {coverage_percentage:.2f}% coverage, "
                   f"confidence: {prediction_certainty:.3f}")
        
        return stats
    
    def _calculate_consensus_confidence(self, mask: np.ndarray, original_image: np.ndarray) -> float:
        """Calcula confian√ßa baseada em consenso entre m√∫ltiplos m√©todos."""
        try:
            # Testa detec√ß√£o com outros m√©todos para valida√ß√£o cruzada
            temp_precision = self.precision_params['enabled']
            self.set_precision_mode(False)  # Usa modo r√°pido para valida√ß√£o
            
            # Testa com m√©todo de cor
            color_mask, color_stats = self.detect_grass_color_based(original_image)
            color_coverage = color_stats['coverage_percentage']
            
            # Testa com m√©todo de textura  
            texture_mask, texture_stats = self.detect_grass_texture_based(original_image)
            texture_coverage = texture_stats['coverage_percentage']
            
            # Restaura modo anterior
            self.set_precision_mode(temp_precision)
            
            # Calcula consenso baseado na consist√™ncia entre m√©todos
            dl_coverage = (np.sum(mask > 0) / (mask.shape[0] * mask.shape[1])) * 100
            
            # M√©trica de consenso - quanto mais pr√≥ximos, maior a confian√ßa
            max_diff = max(
                abs(dl_coverage - color_coverage),
                abs(dl_coverage - texture_coverage),
                abs(color_coverage - texture_coverage)
            )
            
            # Normaliza diferen√ßa para score de consenso (0-1)
            consensus_score = 1.0 / (1.0 + max_diff / 20.0)  # Diferen√ßa de 20% = 50% confian√ßa
            
            # Boost para detec√ß√µes consistentes
            if max_diff < 10:  # Menos de 10% diferen√ßa
                consensus_score *= 1.5
            elif max_diff < 5:  # Menos de 5% diferen√ßa
                consensus_score *= 2.0
                
            return np.clip(consensus_score, 0.1, 1.0)
            
        except Exception as e:
            logger.debug(f"Erro no c√°lculo de consenso: {e}")
            return 0.5  # Confian√ßa neutra se houver erro
    
    def _simulate_deep_learning_prediction(self, normalized_image: np.ndarray) -> np.ndarray:
        """
        Simula uma predi√ß√£o de deep learning inteligente para demonstra√ß√£o.
        Em produ√ß√£o, isso seria substitu√≠do por model.predict().
        """
        # Combina m√∫ltiplas caracter√≠sticas para simular rede neural
        
        # 1. An√°lise de cor no espa√ßo HSV
        hsv = cv2.cvtColor((normalized_image * 255).astype(np.uint8), cv2.COLOR_BGR2HSV)
        hsv_norm = hsv.astype(np.float32) / 255.0
        
        # Canal H (matiz) - peso para tons verdes
        hue_score = np.where(
            (hsv_norm[:, :, 0] > 0.2) & (hsv_norm[:, :, 0] < 0.4),  # Verde
            hsv_norm[:, :, 1] * hsv_norm[:, :, 2],  # Ponderado pela satura√ß√£o e valor
            0.0
        )
        
        # 2. An√°lise de textura usando gradientes
        gray = cv2.cvtColor((normalized_image * 255).astype(np.uint8), cv2.COLOR_BGR2GRAY)
        grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2) / 255.0
        
        # Normaliza textura (vegeta√ß√£o tem textura moderada)
        texture_score = np.where(
            (gradient_magnitude > 0.1) & (gradient_magnitude < 0.6),
            gradient_magnitude,
            gradient_magnitude * 0.3
        )
        
        # 3. An√°lise de intensidade (vegeta√ß√£o n√£o √© muito clara nem muito escura)
        gray_norm = gray.astype(np.float32) / 255.0
        intensity_score = 1.0 - np.abs(gray_norm - 0.4)  # √ìtimo em ~0.4
        intensity_score = np.clip(intensity_score * 2, 0, 1)
        
        # 4. Combina caracter√≠sticas (simula camadas da rede neural)
        feature_1 = hue_score * 0.4 + texture_score * 0.3 + intensity_score * 0.3
        
        # Aplica filtro gaussiano (simula pooling/convolu√ß√£o)
        feature_2 = cv2.GaussianBlur(feature_1, (11, 11), 0)
        
        # "Ativa√ß√£o" n√£o-linear (simula fun√ß√£o de ativa√ß√£o)
        prediction = 1.0 / (1.0 + np.exp(-5 * (feature_2 - 0.4)))  # Sigmoid
        
        # Adiciona um pouco de ru√≠do para simular incerteza do modelo
        noise = np.random.normal(0, 0.05, prediction.shape)
        prediction = np.clip(prediction + noise, 0, 1)
        
        return prediction
    
    def _get_dominant_colors(self, image: np.ndarray, mask: np.ndarray, k: int = 3) -> List[Tuple[int, int, int]]:
        """
        Extrai cores dominantes da √°rea detectada.
        
        Args:
            image: Imagem original
            mask: M√°scara da √°rea de interesse
            k: N√∫mero de clusters
            
        Returns:
            Lista das cores dominantes
        """
        try:
            # Extrai pixels da √°rea mascarada
            masked_pixels = image[mask > 0]
            
            if len(masked_pixels) < k:
                return []
            
            # Aplica K-means clustering
            kmeans = KMeans(n_clusters=k, random_state=42)
            kmeans.fit(masked_pixels)
            
            # Converte cores para int
            colors = []
            for color in kmeans.cluster_centers_:
                colors.append(tuple(int(c) for c in color))
            
            return colors
            
        except Exception as e:
            logger.warning(f"Erro ao extrair cores dominantes: {str(e)}")
            return []
    
    def analyze_grass_density(self, mask: np.ndarray) -> Dict:
        """
        Analisa a densidade e distribui√ß√£o do mato detectado.
        
        Args:
            mask: M√°scara bin√°ria das √°reas detectadas
            
        Returns:
            An√°lise detalhada da densidade
        """
        # Encontra contornos
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if not contours:
            return {
                'num_regions': 0,
                'largest_area': 0,
                'average_area': 0,
                'density_classification': 'none'
            }
        
        # Calcula √°reas
        areas = [cv2.contourArea(contour) for contour in contours]
        total_area = sum(areas)
        
        # Classifica densidade
        image_area = mask.shape[0] * mask.shape[1]
        coverage_ratio = total_area / image_area
        
        if coverage_ratio < 0.1:
            density = 'low'
        elif coverage_ratio < 0.3:
            density = 'medium'
        else:
            density = 'high'
        
        analysis = {
            'num_regions': len(contours),
            'largest_area': max(areas),
            'average_area': np.mean(areas),
            'total_area': total_area,
            'coverage_ratio': coverage_ratio,
            'density_classification': density,
            'area_distribution': {
                'min': min(areas),
                'max': max(areas),
                'std': np.std(areas)
            }
        }
        
        return analysis
    
    def get_detection_confidence(self, stats: Dict) -> float:
        """
        Calcula n√≠vel de confian√ßa da detec√ß√£o.
        
        Args:
            stats: Estat√≠sticas da detec√ß√£o
            
        Returns:
            Confian√ßa entre 0 e 1
        """
        coverage = stats.get('coverage_percentage', 0)
        
        # Baseado na cobertura e m√©todo usado
        if stats.get('method') == 'combined':
            base_confidence = min(coverage / 50.0, 1.0)  # 50% coverage = 100% confidence
        else:
            base_confidence = min(coverage / 70.0, 1.0)  # M√©todos individuais precisam mais cobertura
        
        # Ajusta baseado no n√∫mero de regi√µes detectadas
        if 'individual_stats' in stats:
            color_coverage = stats['individual_stats']['color']['coverage_percentage']
            texture_coverage = stats['individual_stats']['texture']['coverage_percentage']
            
            # Se ambos os m√©todos concordam, aumenta confian√ßa
            agreement = 1.0 - abs(color_coverage - texture_coverage) / 100.0
            base_confidence *= (0.8 + 0.2 * agreement)
        
        return min(base_confidence, 1.0)


if __name__ == "__main__":
    # Teste b√°sico do detector
    detector = GrassDetector()
    
    # Cria imagem de teste
    test_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
    
    # Adiciona algumas √°reas "verdes" para simular mato
    test_image[100:200, 100:300] = [50, 150, 50]  # Verde
    test_image[250:350, 200:400] = [80, 120, 40]  # Verde escuro
    
    # Executa detec√ß√£o
    mask, stats = detector.detect_grass_areas(test_image, method='combined')
    
    print("Estat√≠sticas da detec√ß√£o:")
    print(f"Cobertura: {stats['coverage_percentage']:.2f}%")
    print(f"Confian√ßa: {detector.get_detection_confidence(stats):.2f}")
    
    # Analisa densidade
    density_analysis = detector.analyze_grass_density(mask)
    print(f"Densidade: {density_analysis['density_classification']}")
    print(f"N√∫mero de regi√µes: {density_analysis['num_regions']}")